{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17836627-ba66-47f7-8c7f-718586fd4e4c",
   "metadata": {},
   "source": [
    "Opis problemu\n",
    "\n",
    "Znajdź dowolny zbiór danych (dozwolone języki: angielski, hiszpański, polski, szwedzki) (poza IMDB oraz zbiorami wykorzystywanymi na zajęciach) do analizy sentymentu. Zbiór może mieć 2 lub 3 klasy.\n",
    "\n",
    "Następnie:\n",
    "\n",
    "Oczyść dane i zaprezentuj rozkład klas\n",
    "Zbuduj model analizy sentymenu:\n",
    "z wykorzystaniem sieci rekurencyjnej (LSTM/GRU/sieć dwukierunkowa) innej niż podstawowe RNN\n",
    "z wykorzystaniem sieci CNN\n",
    "z podstawiemiem pre-trained word embeddingów\n",
    "z fine-tuningiem modelu języka (poza podstawowym BERTem)\n",
    "Stwórz funkcję, która będzie korzystała z wytrenowanego modelu i zwracała wynik dla przekazanego pojedynczego zdania (zdań) w postaci komunikatu informującego użytkownika, czy tekst jest nacechowany negatywnie, pozytywnie (czy neutralnie w przypadku 3 klas).\n",
    "\n",
    "Gotowe rozwiązanie zamieść na GitHubie z README. W README zawrzyj: informacje o danych - ich pochodzenie, oraz opis wybranego modelu i instrukcje korzystania z plików.\n",
    "\n",
    "W assigmnencie w Teamsach wrzuć link do repo z rozwiązaniem. W przypadku prywatnego repo upewnij się, że będzie ono widoczne dla dwnuk@pjwstk.edu.pl.\n",
    "\n",
    "TERMIN: jak w Teamsach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b187a4-ee7c-43bf-bc50-aa346013a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "import spacy\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import text,sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout,SpatialDropout1D,GlobalMaxPooling1D, Dense, Conv1D, MaxPooling1D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea1e4a6-472b-4f64-b9f7-092a98b2cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_fwf('train.ft.txt', header = None)\n",
    "test_df = pd.read_fwf('test.ft.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a13521f-55dd-4caa-b41b-238d3cdfeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=0.01, random_state=13)\n",
    "test_df = test_df.sample(frac=0.01, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d750944-d87c-429f-ae22-3ac914f5119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop([2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1afb7e38-d87e-49cc-b724-fd648c4078bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = [\"label\", \"text\"]\n",
    "test_df.columns = [\"label\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37e41ed-7879-4a4c-a9f5-90f878b2b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].str.replace('__label__', '').astype(int)\n",
    "test_df['label'] = test_df['label'].str.replace('__label__', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd63eab-a0ca-428d-8345-98f40003e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text=text.str.lower()\n",
    "  text=text.apply(lambda x: re.sub(r'[0-9]+','',x))\n",
    "  text=text.apply(lambda x: re.sub(r'@mention',' ',x))\n",
    "  text=text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', ' ',x))\n",
    "  text=text.apply(lambda x: re.sub(r\"www.\\[a-z]?\\.?(com)+|[a-z]+\\.(com)\", ' ',x))\n",
    "  text=text.apply(lambda x: re.sub(r\"[_\\,\\>\\(\\-:\\)\\\\\\/\\!\\.\\^\\!\\:\\];='#]\",'',x))\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11952c6-4d12-4fdc-a5b1-2a12b118c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = clean_text(train_df['text'])\n",
    "test_df['text'] = clean_text(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86ec159-937b-4c17-97e4-190d36ff73b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91868 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train_df['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd6f35f-a174-4dea-bacc-1369cba0f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = tokenizer.texts_to_sequences(train_df['text'].values)\n",
    "train_text = pad_sequences(train_text, maxlen=250)\n",
    "\n",
    "y = pd.get_dummies(train_df['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018b24a1-f80b-4cb7-aeb7-c367bc1d918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_text,y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5effdb5-57d8-4d17-b245-2cd22b7fa3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "\n",
    "model_LSTM = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(16),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_LSTM.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76dd50a2-7066-42f2-9f1a-4e22e4f20827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "450/450 [==============================] - 40s 88ms/step - loss: 0.3882 - accuracy: 0.8240 - val_loss: 0.2974 - val_accuracy: 0.8826\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 41s 91ms/step - loss: 0.2094 - accuracy: 0.9219 - val_loss: 0.2612 - val_accuracy: 0.8975\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 43s 95ms/step - loss: 0.1469 - accuracy: 0.9456 - val_loss: 0.2997 - val_accuracy: 0.8906\n"
     ]
    }
   ],
   "source": [
    "history = model_LSTM.fit(x_train, y_train , validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c565bc7-db94-44de-adac-3ee3cd2ea273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 250, 128)          64128     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 62, 128)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 62, 64)            41024     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 15, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 64)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1106226 (4.22 MB)\n",
      "Trainable params: 1106226 (4.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "\n",
    "model_CNN = Sequential([\n",
    "    Embedding(10000, 100, input_length=train_text.shape[1]),\n",
    "    Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'),\n",
    "    MaxPooling1D(pool_size=4),\n",
    "    Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
    "    MaxPooling1D(pool_size=4),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_CNN.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace05bb7-09b9-4738-a8f7-97b3df161d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "450/450 [==============================] - 15s 34ms/step - loss: 0.3896 - accuracy: 0.8138 - val_loss: 0.2506 - val_accuracy: 0.8974\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 16s 35ms/step - loss: 0.1998 - accuracy: 0.9278 - val_loss: 0.2757 - val_accuracy: 0.8856\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 16s 36ms/step - loss: 0.1183 - accuracy: 0.9606 - val_loss: 0.3129 - val_accuracy: 0.8892\n"
     ]
    }
   ],
   "source": [
    "history = model_CNN.fit(x_train, y_train, epochs = 3, batch_size = 64, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67c17967-de23-44ea-a93d-dfb72f5ce31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-trained word embedding\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "        \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56a4ad76-f7ff-48eb-85a1-636fa54d4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = Sequential([\n",
    "    Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=train_text.shape[1], trainable=False),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(16),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_embedding.compile(loss='binary_crossentropy', \n",
    "                        optimizer='adam', \n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5308d5c4-638d-425f-9991-152d2c937d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "450/450 [==============================] - 31s 67ms/step - loss: 0.5581 - accuracy: 0.7195 - val_loss: 0.4638 - val_accuracy: 0.7919\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 30s 66ms/step - loss: 0.4379 - accuracy: 0.8019 - val_loss: 0.4099 - val_accuracy: 0.8278\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 30s 66ms/step - loss: 0.3796 - accuracy: 0.8347 - val_loss: 0.3552 - val_accuracy: 0.8515\n"
     ]
    }
   ],
   "source": [
    "history = model_embedding.fit(x_train, y_train, epochs=3, batch_size=64, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c405610b-cad4-40e3-af6d-47c851ff461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, text):\n",
    "    cleaned_text = clean_text(pd.Series([text]))\n",
    "    text_sequence = tokenizer.texts_to_sequences(cleaned_text)\n",
    "    padded_sequence = pad_sequences(text_sequence, maxlen=250)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "\n",
    "    if prediction[0][0] > prediction[0][1]:\n",
    "        return \"Recenzja nacechowana negatywnie\"\n",
    "    else:\n",
    "        return \"Recenzja nacechowana pozytywnie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f1f0cc1-0f29-4c33-9f32-a05757892e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 275ms/step\n",
      "Recenzja nacechowana negatywnie\n"
     ]
    }
   ],
   "source": [
    "result_lstm = predict_sentiment(model_LSTM, tokenizer, \"terrible product\")\n",
    "print(result_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adf16f3f-11f6-4d05-bff3-966dd6831df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "Recenzja nacechowana pozytywnie\n"
     ]
    }
   ],
   "source": [
    "result_cnn = predict_sentiment(model_CNN, tokenizer, \"great tv, everything works fine\")\n",
    "print(result_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3b2d620-abb4-4e95-91f5-cad532f26296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step\n",
      "Recenzja nacechowana negatywnie\n"
     ]
    }
   ],
   "source": [
    "result_embedding = predict_sentiment(model_embedding, tokenizer, \"awful service i returned the product after a week\")\n",
    "print(result_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
